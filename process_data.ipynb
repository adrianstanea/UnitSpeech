{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conf.hydra_config import (\n",
    "    TrainingUnitEncoderConfig_STEP1,\n",
    ")\n",
    "import logging\n",
    "import torch\n",
    "\n",
    "\n",
    "cfg = TrainingUnitEncoderConfig_STEP1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and cfg.train.on_GPU else \"cpu\")\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization of Mel-Spectrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speaker embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/astanea/.cache/torch/hub/s3prl_s3prl_main\n",
      "/home/astanea/anaconda3/envs/UnitSpeech308_OK/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-04-17 21:07:29 | INFO | s3prl.util.download | Requesting URL: https://huggingface.co/s3prl/converted_ckpts/resolve/main/wavlm_large.pt\n",
      "2024-04-17 21:07:29 | INFO | s3prl.util.download | Using URL's local file: /home/astanea/.cache/s3prl/download/f2d5200177fd6a33b278b7b76b454f25cd8ee866d55c122e69fccf6c7467d37d.wavlm_large.pt\n",
      "2024-04-17 21:07:36 | INFO | s3prl.upstream.wavlm.WavLM | WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}\n",
      "/home/astanea/anaconda3/envs/UnitSpeech308_OK/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=[], unexpected_keys=['loss_calculator.projection.weight'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unitspeech.speaker_encoder.ecapa_tdnn import ECAPA_TDNN\n",
    "\n",
    "\n",
    "spkr_embedder = ECAPA_TDNN(feat_dim=cfg.spkr_embedder.feat_dim,\n",
    "                        channels=cfg.spkr_embedder.channels,\n",
    "                        emb_dim=cfg.spkr_embedder.spk_emb_dim,\n",
    "                        feat_type=cfg.spkr_embedder.feat_type,\n",
    "                        sr=cfg.spkr_embedder.sr,\n",
    "                        feature_selection=cfg.spkr_embedder.feature_selection,\n",
    "                        update_extract=cfg.spkr_embedder.update_extract,\n",
    "                        config_path=cfg.spkr_embedder.config_path).to(device).eval()\n",
    "\n",
    "state_dict = torch.load(cfg.spkr_embedder.checkpoint,\n",
    "                        map_location=lambda loc, storage: loc)\n",
    "spkr_embedder.load_state_dict(state_dict[\"model\"], strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unitspeech.util import parse_filelist\n",
    "import os\n",
    "import librosa\n",
    "import torchaudio\n",
    "from functools import lru_cache\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def get_resampler(orig_sr, target_sr):\n",
    "    return torchaudio.transforms.Resample(orig_sr, target_sr).cuda()\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def load_and_process_wav(filepath, device):\n",
    "    wav, sr = librosa.load(filepath)\n",
    "    wav = torch.FloatTensor(wav).to(device)\n",
    "    if sr != 16_000:\n",
    "        resample_fn = get_resampler(sr, 16000)\n",
    "        wav = resample_fn(wav)\n",
    "    return wav\n",
    "\n",
    "def save_mean_emb(crnt_spkr_mean, crnt_speaker, all_mean_embs, dataset_name):\n",
    "    all_mean_embs[int(crnt_speaker)] = crnt_spkr_mean.unsqueeze(1)\n",
    "    os.makedirs(f\"resources/{dataset_name}/speaker_embs\", exist_ok=True)\n",
    "    torch.save(crnt_spkr_mean, f\"resources/{dataset_name}/speaker_embs/{crnt_speaker}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_spkr_embs(filelist_path, dataset_name):\n",
    "    print(f\"Loading filelist from {filelist_path}\")\n",
    "    filelist = parse_filelist(filelist_path, split_char='|')\n",
    "\n",
    "    global all_mean_embs\n",
    "    crnt_speaker = -1\n",
    "    num_samples = 0\n",
    "\n",
    "    crnt_spkr_mean = torch.zeros(cfg.spkr_embedder.spk_emb_dim).unsqueeze(0).to(device)\n",
    "    is_first_sample = True\n",
    "\n",
    "    for idx, line in enumerate(filelist, start=1):\n",
    "        filepath, text, spk_id = line[0], line[1], line[2]\n",
    "        if idx % 10 == 0 or idx == 0:\n",
    "            print(f\"Processing line ({idx}|{len(filelist)})\")\n",
    "\n",
    "        if crnt_speaker == -1:\n",
    "            print(f\"First speaker was: {spk_id}\")\n",
    "            crnt_speaker = spk_id\n",
    "\n",
    "        if spk_id != crnt_speaker: # New speaker detected\n",
    "            print(f\"Number of sample: {num_samples}\")\n",
    "            save_mean_emb(crnt_spkr_mean, crnt_speaker, all_mean_embs, dataset_name=dataset_name)\n",
    "            # Reset for new speaker ID\n",
    "            crnt_spkr_mean = torch.zeros(cfg.spkr_embedder.spk_emb_dim).unsqueeze(0).to(device)\n",
    "            is_first_sample = True\n",
    "            num_samples = 0\n",
    "            crnt_speaker = spk_id\n",
    "\n",
    "            print(f\"Speaker change to {spk_id} | current line = {idx}\")\n",
    "\n",
    "        wav = load_and_process_wav(filepath, device)\n",
    "        emb = spkr_embedder(wav.unsqueeze(0))\n",
    "        if is_first_sample:\n",
    "            crnt_spkr_mean = emb\n",
    "            is_first_sample = False\n",
    "        else:\n",
    "            crnt_spkr_mean = torch.mean(torch.stack([crnt_spkr_mean, emb]), dim=0)\n",
    "        num_samples += 1\n",
    "    return all_mean_embs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libri-TTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From all the speakers in the Libri-TTS dataset, we extract the speaker embeddings using the pre-trained speaker encoder model.\n",
    "- Each speaker embedding is a 256-dimensional vector. The vector contains the mean value of the embeddings of all the utterances of the speaker.\n",
    "- Speaker ID's range from 1 to 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conf.hydra_config import LibriTTSConfig as dataset_cfg\n",
    "\n",
    "all_mean_embs = {}\n",
    "all_mean_embs = get_mean_spkr_embs(dataset_cfg.train_filelist_path,\n",
    "                   dataset_cfg.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(all_mean_embs, f\"resources/{dataset_cfg.name}/speaker_embs/speaker_embs.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LJ-Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conf.hydra_config import LJSPeechConfig as dataset_cfg\n",
    "\n",
    "all_mean_embs = {}\n",
    "all_mean_embs = get_mean_spkr_embs(dataset_cfg.train_filelist_path,\n",
    "                   dataset_cfg.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(all_mean_embs, f\"resources/{dataset_cfg.name}/speaker_embs/speaker_embs.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UnitSpeech308_OK",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
