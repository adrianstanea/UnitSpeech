{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find mel min/max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conf.hydra_config import (\n",
    "    TrainingUnitEncoderConfig_STEP1,\n",
    ")\n",
    "import logging\n",
    "import torch\n",
    "\n",
    "\n",
    "cfg = TrainingUnitEncoderConfig_STEP1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and cfg.train.on_GPU else \"cpu\")\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LibriTTS'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.dataset.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([80]),\n",
       " tensor([-11.5129, -11.5129, -11.5129, -11.5129, -11.5129, -11.5129, -11.5129,\n",
       "         -11.5129, -11.5129, -11.5129, -11.5129, -11.5129, -11.5129, -11.5129,\n",
       "         -11.5129, -11.5129, -11.5129, -11.5129, -11.5129, -11.5129, -11.5129,\n",
       "         -11.5129, -11.5129, -11.5129, -11.5129, -11.5129, -11.5129, -11.5129,\n",
       "         -11.5129, -11.5129, -11.5129, -11.5129, -11.5129, -11.5129, -11.5129,\n",
       "         -11.5129, -11.5129, -11.5129, -11.5129, -11.5129, -11.5129, -11.5129,\n",
       "         -11.5129, -11.5129, -11.5129, -11.5129, -11.5129, -11.5129, -11.5129,\n",
       "         -11.5129, -11.5129, -11.5129, -11.5129, -11.5129, -11.5129, -11.5129,\n",
       "         -11.5129, -11.5129, -11.5129, -11.5129, -11.5129, -11.5129, -11.5129,\n",
       "         -11.5129, -11.5129, -11.5129, -11.5129, -11.5129, -11.5129, -11.5129,\n",
       "         -11.5129, -11.5129, -11.5129, -11.5129, -11.5129, -11.5129, -11.5129,\n",
       "         -11.5129, -11.5129, -11.5129], device='cuda:0'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mel_min = torch.load(cfg.dataset.mel_min_path).to(device)\n",
    "mel_min.shape, mel_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([80]),\n",
       " tensor([-11.5129, -11.5129, -11.5129, -11.5129, -11.5129, -11.5129, -11.5129,\n",
       "         -11.5129, -11.5129, -11.5129, -11.5129, -11.5129, -11.5129, -11.5129,\n",
       "         -11.5129, -11.5129, -11.5129, -11.5129, -11.5129, -11.5129, -11.5129,\n",
       "         -11.5129, -11.5129, -11.5129, -11.5129, -11.5129, -11.5129, -11.5129,\n",
       "         -11.5129, -11.5129, -11.5129, -11.5129, -11.5129, -11.5129, -11.5129,\n",
       "         -11.5129, -11.5129, -11.5129, -11.5129, -11.5129, -11.5129, -11.5129,\n",
       "         -11.5129, -11.5129, -11.5129, -11.5129, -11.5129, -11.5129, -11.5129,\n",
       "         -11.5129, -11.5129, -11.5129, -11.5129, -11.5129, -11.5129, -11.5129,\n",
       "         -11.5129, -11.5129, -11.5129, -11.5129, -11.5129, -11.5129, -11.5129,\n",
       "         -11.5129, -11.5129, -11.5129, -11.5129, -11.5129, -11.5129, -11.5129,\n",
       "         -11.5129, -11.5129, -11.5129, -11.5129, -11.5129, -11.5129, -11.5129,\n",
       "         -11.5129, -11.5129, -11.5129]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_mel_min = torch.load(\"unitspeech/checkpoints/mel_min.pt\")\n",
    "original_mel_min.shape, original_mel_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (v1, v2) in enumerate(zip(mel_min.flatten(), original_mel_min.flatten())):\n",
    "    if v1 == v2:\n",
    "        continue\n",
    "        print(f\"[EQUAL]At index {i}, mel_min is {v1} while original_mel_min is {v2}\")\n",
    "    if v1 != v2:\n",
    "        print(f\"[NOT EQUAL]At index {i}, mel_min is {v1} while original_mel_min is {v2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([80]),\n",
       " tensor([ 2.2396,  1.7412,  2.0047,  1.8635,  1.6160,  1.6770,  1.7164,  1.6729,\n",
       "          1.9186,  1.9919,  1.9429,  2.0060,  2.0480,  2.1126,  2.1814,  2.0440,\n",
       "          1.9951,  1.9164,  2.1956,  1.9244,  1.9685,  1.8858,  1.6691,  1.6388,\n",
       "          2.1799,  1.7118,  1.5506,  1.5032,  1.6166,  1.7240,  1.9226,  1.9259,\n",
       "          1.6882,  1.5648,  1.2986,  1.4236,  1.3465,  1.0905,  1.0518,  1.1202,\n",
       "          1.1056,  0.9698,  0.9007,  0.9808,  0.9551,  0.9867,  0.7309,  0.8902,\n",
       "          0.5384,  0.7088,  0.5863,  0.4398,  0.3234,  0.4925,  0.6535,  0.4604,\n",
       "          0.3580,  0.6003,  0.7013,  0.7245,  0.7301,  0.7176,  0.4676,  1.1647,\n",
       "          0.9116,  0.5581,  0.5944,  0.0812,  0.6586,  0.3508,  0.1807,  0.2859,\n",
       "          0.5973,  0.0491, -0.0368, -0.0094,  0.1095, -0.0434,  0.0059,  0.1408],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mel_max = torch.load(cfg.dataset.mel_max_path).to(device)\n",
    "mel_max.shape, mel_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([80]),\n",
       " tensor([2.3678, 1.9858, 2.0981, 2.0714, 1.9077, 2.2800, 2.3345, 2.3407, 2.1605,\n",
       "         2.2150, 2.2823, 2.1748, 2.1844, 2.3054, 2.3514, 2.2928, 2.1999, 2.2282,\n",
       "         2.2321, 2.0936, 2.1599, 2.1016, 2.1892, 2.0314, 2.1798, 2.2661, 2.0354,\n",
       "         2.1316, 2.1162, 1.9804, 1.9803, 1.9256, 1.7991, 1.8490, 2.0034, 1.6449,\n",
       "         1.5683, 1.7728, 1.6321, 1.5499, 1.6727, 1.5721, 1.3449, 1.4617, 1.3555,\n",
       "         1.4562, 1.1820, 1.0871, 1.1762, 0.9916, 0.9349, 0.9958, 1.1382, 1.0810,\n",
       "         1.1063, 0.9895, 0.9962, 1.4600, 1.4202, 1.1430, 1.3783, 0.9589, 0.9599,\n",
       "         1.1538, 0.9055, 0.6158, 0.8025, 0.7124, 0.6566, 0.8171, 0.8327, 0.7887,\n",
       "         0.8496, 0.6723, 0.6793, 0.5739, 0.9136, 0.8758, 0.8724, 0.6244]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_mel_max = torch.load(\"unitspeech/checkpoints/mel_max.pt\")\n",
    "original_mel_max.shape, original_mel_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n",
      "Continue\n"
     ]
    }
   ],
   "source": [
    "for i, (v1, v2) in enumerate(zip(mel_max.flatten(), original_mel_max.flatten())):\n",
    "    if v1 == v2:\n",
    "        print(f\"[EQUAL]At index {i}, mel_max is {v1} while original_mel_max is {v2}\")\n",
    "    if v1 != v2:\n",
    "        print(f\"[NOT EQUAL]At index {i}, mel_max is {v1} while original_mel_max is {v2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import TextMelSpeakerDataset\n",
    "\n",
    "\n",
    "train_dataset = TextMelSpeakerDataset(filelist_path=cfg.dataset.train_filelist_path,\n",
    "                                        random_seed=cfg.train.seed,\n",
    "                                        add_blank=cfg.data.add_blank,\n",
    "                                        n_fft=cfg.data.n_fft,\n",
    "                                        n_mels=cfg.data.n_feats,\n",
    "                                        sample_rate=cfg.data.sampling_rate,\n",
    "                                        hop_length=cfg.data.hop_length,\n",
    "                                        win_length=cfg.data.win_length,\n",
    "                                        f_min=cfg.data.mel_fmin,\n",
    "                                        f_max=cfg.data.mel_fmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "tensor = torch.arange(10).unsqueeze(1).repeat(1, 256)\n",
    "# tensor = torch.full((10, 256), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 256])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([257, 256])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weigths = torch.arange(257).float().unsqueeze(1).repeat(1, 256)\n",
    "weigths.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "embeddings = nn.Embedding.from_pretrained(weigths, freeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "         2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "         2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "         2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "         2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "         2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "         2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "         2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "         2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "         2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "         2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "         2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "         2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "         2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "         2., 2., 2., 2.],\n",
       "        [3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
       "         3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
       "         3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
       "         3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
       "         3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
       "         3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
       "         3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
       "         3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
       "         3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
       "         3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
       "         3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
       "         3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
       "         3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
       "         3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
       "         3., 3., 3., 3.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = torch.LongTensor([2, 3])\n",
    "embeddings(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "weigths = torch.randn(257, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def load_speaker_embs(embs_path: str, normalize=True):\n",
    "    \"\"\"Load speaker embeddings from .pt files in a directory.\n",
    "\n",
    "    Args:\n",
    "        embs_path (str): Folder with .pt files containing speaker embeddings for the current dataset\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Tensor containing all speaker embeddings\n",
    "    \"\"\"\n",
    "    embs = []\n",
    "    for spkr_id in sorted(os.listdir(embs_path)):\n",
    "        if spkr_id.endswith('.pt'):\n",
    "            spkr_emb = torch.load(os.path.join(embs_path, spkr_id))\n",
    "\n",
    "            if normalize:\n",
    "                spkr_emb = spkr_emb / spkr_emb.norm()\n",
    "            embs.append(spkr_emb)\n",
    "        else:\n",
    "            raise ValueError(f\"Speaker embedding file {spkr_id} is not a .pt file.\")\n",
    "    return torch.stack(embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embs = load_speaker_embs(os.path.join(\"unitspeech/checkpoints/embs/\", \"LJSpeech\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_embeddings = torch.nn.Embedding.from_pretrained(test_embs).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000, device='cuda:0')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speaker_embeddings(torch.LongTensor([0]).to(\"cuda\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UnitSpeech308_OK",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
