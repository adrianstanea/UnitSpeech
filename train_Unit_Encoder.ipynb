{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import librosa\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torchaudio\n",
    "from tqdm import tqdm\n",
    "\n",
    "from unitspeech.unitspeech import UnitSpeech\n",
    "from unitspeech.encoder import Encoder\n",
    "from unitspeech.speaker_encoder.ecapa_tdnn import ECAPA_TDNN_SMALL\n",
    "from unitspeech.textlesslib.textless.data.speech_encoder import SpeechEncoder\n",
    "from unitspeech.util import HParams, fix_len_compatibility, process_unit, generate_path, sequence_mask\n",
    "from unitspeech.vocoder.env import AttrDict\n",
    "from unitspeech.vocoder.meldataset import mel_spectrogram\n",
    "from unitspeech.vocoder.models import BigVGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"reference_path\": \"path_to_reference_audio\",\n",
    "    \"encoder_path\": \"unitspeech/checkpoints/unit_encoder.pt\",\n",
    "    \"decoder_path\": \"unitspeech/checkpoints/pretrained_decoder.pt\",\n",
    "    \"speaker_encoder_path\": \"unitspeech/speaker_encoder/checkpts/speaker_encoder.pt\",\n",
    "    \"config_path\": \"unitspeech/checkpoints/finetune.json\",\n",
    "    \"output_decoder_path\": \"unitspeech/outputs/finetuned_decoder.pt\",\n",
    "    \"n_iters\": 500,\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"fp16_run\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'unitspeech/checkpoints/finetune.json'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.get(\"config_path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(args.get(\"config_path\"), \"r\") as f:\n",
    "    data = f.read()\n",
    "config = json.loads(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': {'n_units': 1000, 'n_feats': 80, 'n_fft': 1024, 'hop_length': 256, 'win_length': 1024, 'sampling_rate': 22050, 'mel_fmin': 0.0, 'mel_fmax': 8000.0}, 'encoder': {'n_channels': 192, 'filter_channels': 768, 'n_layers': 6, 'kernel_size': 3, 'p_dropout': 0.1, 'n_heads': 2, 'window_size': 4}, 'decoder': {'dim': 128, 'dim_mults': [1, 2, 4, 8], 'pe_scale': 1000, 'beta_min': 0.05, 'beta_max': 20.0, 'spk_emb_dim': 256}, 'train': {'out_size_second': 2, 'vocoder_config_path': 'unitspeech/vocoder/checkpts/bigvgan-config.json', 'vocoder_ckpt_path': 'unitspeech/vocoder/checkpts/bigvgan.pt'}}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hps = HParams(**config)\n",
    "hps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment_size = fix_len_compatibility(\n",
    "    hps.train.out_size_second * hps.data.sampling_rate // hps.data.hop_length,\n",
    "    len(hps.decoder.dim_mults) - 1\n",
    ")\n",
    "segment_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_units = hps.data.n_units\n",
    "num_units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speaker Encoder\n",
    "\n",
    "- Load the pre-trained speaker encoder model\n",
    "- Set it to evaluation mode: no training is needed -> KEEP FROZEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/astanea/.cache/torch/hub/s3prl_s3prl_main\n",
      "/home/astanea/anaconda3/envs/UnitSpeech308_OK/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-03-31 13:50:40 | INFO | s3prl.util.download | Requesting URL: https://huggingface.co/s3prl/converted_ckpts/resolve/main/wavlm_large.pt\n",
      "2024-03-31 13:50:40 | INFO | s3prl.util.download | Using URL's local file: /home/astanea/.cache/s3prl/download/f2d5200177fd6a33b278b7b76b454f25cd8ee866d55c122e69fccf6c7467d37d.wavlm_large.pt\n",
      "2024-03-31 13:50:44 | INFO | s3prl.upstream.wavlm.WavLM | WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "spk_embedder = ECAPA_TDNN_SMALL(feat_dim=1024,\n",
    "                                feat_type=\"wavlm_large\", # NOTE: see other feature types\n",
    "                                config_path=None)\n",
    "state_dict = torch.load(args.speaker_encoder_path,\n",
    "                        map_location=lambda storage, loc: storage)\n",
    "spk_embedder.load_state_dict(state_dict['model'],\n",
    "                             strict=False)\n",
    "_ = spk_embedder.cuda().eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit extracter -> from textlesslib\n",
    "\n",
    "- The speech encoder should just generated units, load and use it as it it: FREEZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-31 13:24:21 | INFO | fairseq.tasks.hubert_pretraining | current directory is /home/astanea/dev/UnitSpeech\n",
      "2024-03-31 13:24:21 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': '/checkpoint/annl/s2st/data/voxpopuli/mHuBERT/en_es_fr', 'fine_tuning': False, 'labels': ['km'], 'label_dir': '/checkpoint/wnhsu/experiments/hubert/kmeans/mhubert_vp_en_es_fr_it2_400k/en_es_fr.layer9.km500', 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}\n",
      "2024-03-31 13:24:21 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': True, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.1, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}\n"
     ]
    }
   ],
   "source": [
    "dense_model_name = \"mhubert-base-vp_en_es_fr\"\n",
    "quantizer_name, vocab_size = \"kmeans\", 1000 # TODO: 1000 might be n_units hyperparameter from data.n_units\n",
    "\n",
    "unit_extractor = SpeechEncoder.by_name(\n",
    "    dense_model_name=dense_model_name,\n",
    "    quantizer_model_name=quantizer_name,\n",
    "    vocab_size=vocab_size,\n",
    "    deduplicate=True,\n",
    "    need_f0=False\n",
    ")\n",
    "_ = unit_extractor.cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: See loading wav, and preprocessing: normalization, extract speaker embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_mel_spec(mel):\n",
    "    mel_min = mel.min(-1, keepdim=True)[0]\n",
    "    mel_max = mel.max(-1, keepdim=True)[0]\n",
    "    \n",
    "    mel = (mel - mel_min) / (mel_max - mel_min) * 2 - 1 # Interval: [-1, 1]\n",
    "    \n",
    "    return (mel - mel_min) / (mel_max - mel_min) * 2 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_speaker_embedding(spk_embedder, wav):\n",
    "    with torch.no_grad():\n",
    "        spk_embedding = spk_embedder(wav)\n",
    "        spk_embedding = spk_embedding / spk_embedding.norm()\n",
    "    \n",
    "    return spk_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features():\n",
    "    wav, sr = librosa.load(args.get(\"reference_path\"))\n",
    "    wav = torch.FloatTensor(wav).unsqueeze(0) # Add batch dimension: (1, num_samples)\n",
    "    \n",
    "    # (batch_dim, n_mels, n_frames)\n",
    "    mel = mel_spectrogram(wav,\n",
    "                        hps.data.n_fft,\n",
    "                        hps.data.n_feats,\n",
    "                        hps.data.sampling_rate,\n",
    "                        hps.data.hop_length,\n",
    "                        hps.data.win_length,\n",
    "                        hps.data.mel_fmin,\n",
    "                        hps.data.mel_fmax,\n",
    "                        center=False)\n",
    "    mel = normalize_mel_spec(mel).cuda()\n",
    "\n",
    "    # Sr was at 22050, resample to 16000\n",
    "    resample_fn = torchaudio.transforms.Resample(sr, 16000).cuda()\n",
    "    wav = resample_fn(wav.cuda())\n",
    "    \n",
    "    # Extract speaker embedding with Ecapa-TDNN\n",
    "    spk_embedding = get_speaker_embedding(spk_embedder, wav)\n",
    "\n",
    "    # Extract units with dense model\n",
    "    encoded = unit_extractor(wav.to(\"cuda\"))\n",
    "    unit, duration = process_unit(encoded, hps.data.sampling_rate, hps.data.hop_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Training UnitEncoder model => freeze decoder (GradTTS) which should be trained apriori, then adapt the unit encoder to represent the units in the same latent space the text encoder does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unitspeech = UnitSpeech(\n",
    "    n_feats=hps.data.n_feats,\n",
    "    **hps.decoder\n",
    ")\n",
    "\n",
    "decoder_dict = torch.load(args.get(\"decoder_path\"), map_location=lambda loc, storage: loc)\n",
    "unitspeech.load_state_dict(decoder_dict['model'])\n",
    "_ = unitspeech.cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params=unitspeech.parameters(),\n",
    "                             lr=args.get(\"learning_rate\"))\n",
    "\n",
    "if args.fp16_run:\n",
    "    scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_encoder = Encoder(\n",
    "    n_vocab=num_units,\n",
    "    n_feats=hps.data.n_feats,\n",
    "    **hps.encoder\n",
    ")\n",
    "_ = unit_encoder.cuda().train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = next(unit_encoder.parameters()).device\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, hps.train.n_epochs + 1):\n",
    "    unitspeech.eval()\n",
    "    unit_encoder.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UnitSpeech308_OK",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
