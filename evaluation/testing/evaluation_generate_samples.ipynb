{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate speaker embeddings for each one of the 5 speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/unitspeech_3.8/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import IPython.display as ipd\n",
    "import json\n",
    "import librosa\n",
    "import os\n",
    "\n",
    "# To prevent the path from becoming corrupted when this cell is executed more than once.\n",
    "try:\n",
    "    path\n",
    "except:\n",
    "    path = \"../\"\n",
    "    os.chdir(path)\n",
    "    \n",
    "import phonemizer\n",
    "import random\n",
    "from scipy.io.wavfile import write\n",
    "import torch\n",
    "import torchaudio\n",
    "from tqdm import tqdm\n",
    "from transformers import HubertModel\n",
    "\n",
    "from unitspeech.unitspeech import UnitSpeech\n",
    "from unitspeech.duration_predictor import DurationPredictor\n",
    "from unitspeech.encoder import Encoder\n",
    "from unitspeech.speaker_encoder.ecapa_tdnn import ECAPA_TDNN_SMALL\n",
    "from unitspeech.text import cleaned_text_to_sequence, phonemize, symbols\n",
    "from unitspeech.textlesslib.textless.data.speech_encoder import SpeechEncoder\n",
    "from unitspeech.util import HParams, fix_len_compatibility, intersperse, process_unit, generate_path, sequence_mask\n",
    "from unitspeech.vocoder.env import AttrDict\n",
    "from unitspeech.vocoder.meldataset import mel_spectrogram\n",
    "from unitspeech.vocoder.models import BigVGAN\n",
    "\n",
    "from conf.hydra_config import (\n",
    "    MainConfig,\n",
    ")\n",
    "import pandas as pd\n",
    "\n",
    "import soundfile as sf\n",
    "\n",
    "from unitspeech.util import (\n",
    "    fix_len_compatibility,\n",
    "    save_plot,\n",
    "    sequence_mask,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running from /workspace/local\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "cfg = MainConfig\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and cfg.train.on_GPU else \"cpu\")\n",
    "\n",
    "print(f\"Running from {os.getcwd()}\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['path', 'transcript', 'speaker_id']\n",
    "reference_speech_samples = pd.read_csv('reference_speech_samples.csv', delimiter=\"|\", header=None, names=column_names)\n",
    "reference_speech_samples\n",
    "eval_speech_samples = pd.read_csv('evaluation.csv', delimiter=\"|\", header=None, names=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     1\n",
       "1    19\n",
       "2    39\n",
       "3    20\n",
       "4    10\n",
       "Name: speaker_id, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_speech_samples[\"speaker_id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and process reference speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_config_path = \"unitspeech/checkpoints/finetune.json\"\n",
    "with open(finetune_config_path, \"r\") as f:\n",
    "    data = f.read()\n",
    "finetune_config = json.loads(data)\n",
    "\n",
    "\n",
    "fp16_run = False\n",
    "learning_rate = 2e-5\n",
    "# Runtime HYPERPARAMS\n",
    "num_downsamplings_in_unet = len(cfg.decoder.dim_mults) - 1\n",
    "out_size = fix_len_compatibility(\n",
    "    cfg.train.out_size_second * cfg.data.sampling_rate // cfg.data.hop_length, num_downsamplings_in_unet=num_downsamplings_in_unet\n",
    ")\n",
    "\n",
    "hps_finetune = HParams(**finetune_config)\n",
    "\n",
    "segment_size = fix_len_compatibility(\n",
    "    hps_finetune.train.out_size_second * hps_finetune.data.sampling_rate // hps_finetune.data.hop_length,\n",
    "    len(hps_finetune.decoder.dim_mults) - 1\n",
    ")\n",
    "\n",
    "speaker_encoder_path = \"/checkpoints/EVALUATION/speaker_encoder/checkpts/speaker_encoder.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Vocoder...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing weight norm...\n",
      "Initializing Speaker Encoder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/s3prl_s3prl_main\n",
      "2024-06-19 10:06:28 | INFO | s3prl.util.download | Requesting URL: https://huggingface.co/s3prl/converted_ckpts/resolve/main/wavlm_large.pt\n",
      "2024-06-19 10:06:28 | INFO | s3prl.util.download | Using URL's local file: /root/.cache/s3prl/download/f2d5200177fd6a33b278b7b76b454f25cd8ee866d55c122e69fccf6c7467d37d.wavlm_large.pt\n",
      "2024-06-19 10:06:29 | INFO | s3prl.upstream.wavlm.WavLM | WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}\n",
      "/root/miniconda3/envs/unitspeech_3.8/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Unit Extracter...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-19 10:06:38 | INFO | fairseq.tasks.hubert_pretraining | current directory is /workspace/local\n",
      "2024-06-19 10:06:38 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': '/checkpoint/annl/s2st/data/voxpopuli/mHuBERT/en_es_fr', 'fine_tuning': False, 'labels': ['km'], 'label_dir': '/checkpoint/wnhsu/experiments/hubert/kmeans/mhubert_vp_en_es_fr_it2_400k/en_es_fr.layer9.km500', 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}\n",
      "2024-06-19 10:06:38 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': True, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.1, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}\n"
     ]
    }
   ],
   "source": [
    "# TODO Vocoder - MISSING FILES\n",
    "print('Initializing Vocoder...')\n",
    "with open(hps_finetune.train.vocoder_config_path) as f:\n",
    "    h = AttrDict(json.load(f))\n",
    "vocoder = BigVGAN(h)\n",
    "vocoder.load_state_dict(torch.load(hps_finetune.train.vocoder_ckpt_path, map_location=lambda loc, storage: loc)['generator'])\n",
    "_ = vocoder.cuda().eval()\n",
    "vocoder.remove_weight_norm()\n",
    "\n",
    "\n",
    "# Speaker Encoder for extracting speaker embedding\n",
    "print('Initializing Speaker Encoder...')\n",
    "spk_embedder = ECAPA_TDNN_SMALL(feat_dim=1024, feat_type=\"wavlm_large\", config_path=None)\n",
    "state_dict = torch.load(speaker_encoder_path, map_location=lambda storage, loc: storage)\n",
    "spk_embedder.load_state_dict(state_dict['model'], strict=False)\n",
    "_ = spk_embedder.cuda().eval()\n",
    "\n",
    "\n",
    "# Unit Extractor for extraction unit and duration, which are used for finetuning\n",
    "print('Initializing Unit Extracter...')\n",
    "unit_extractor = SpeechEncoder.by_name(dense_model_name=cfg.unit_extractor.dense_model_name,\n",
    "                                        quantizer_model_name=cfg.unit_extractor.quantizer_name,\n",
    "                                        vocab_size=cfg.unit_extractor.vocab_size,\n",
    "                                        deduplicate=cfg.unit_extractor.deduplicate,\n",
    "                                        need_f0=cfg.unit_extractor.need_f0)\n",
    "_ = unit_extractor.cuda().eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TTS modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder = Encoder(\n",
    "    n_vocab=cfg.encoder.n_vocab,\n",
    "    n_feats=cfg.data.n_feats,\n",
    "    n_channels=cfg.encoder.n_channels,\n",
    "    filter_channels=cfg.encoder.filter_channels,\n",
    "    n_heads=cfg.encoder.n_heads,\n",
    "    n_layers=cfg.encoder.n_layers,\n",
    "    kernel_size=cfg.encoder.kernel_size,\n",
    "    p_dropout=cfg.encoder.p_dropout,\n",
    "    window_size=cfg.encoder.window_size,\n",
    ")\n",
    "if not os.path.exists(cfg.encoder.checkpoint):\n",
    "    raise FileNotFoundError(f\"Checkpoint for encoder not found: {cfg.encoder.checkpoint}\")\n",
    "text_encoder_dict = torch.load(cfg.encoder.checkpoint, map_location=lambda loc, storage: loc)\n",
    "text_encoder.load_state_dict(text_encoder_dict[\"model\"])\n",
    "_ = text_encoder.cuda().eval()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_predictor = DurationPredictor(\n",
    "    in_channels=cfg.duration_predictor.in_channels,\n",
    "    filter_channels=cfg.duration_predictor.filter_channels,\n",
    "    kernel_size=cfg.duration_predictor.kernel_size,\n",
    "    p_dropout=cfg.duration_predictor.p_dropout,\n",
    "    spk_emb_dim=cfg.duration_predictor.spk_emb_dim,\n",
    ")\n",
    "if not os.path.exists(cfg.duration_predictor.checkpoint):\n",
    "    raise FileNotFoundError(f\"Checkpoint for duration predictor not found: {cfg.duration_predictor.checkpoint}\")\n",
    "duration_predictor_dict = torch.load(cfg.duration_predictor.checkpoint, map_location=lambda loc, storage: loc)\n",
    "duration_predictor.load_state_dict(duration_predictor_dict[\"model\"])\n",
    "_ = duration_predictor.cuda().eval()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_encoder_path = \"unitspeech/checkpoints/unit_encoder.pt\"\n",
    "unit_encoder = Encoder(n_vocab=cfg.data.n_units,\n",
    "                    n_feats=cfg.data.n_feats,\n",
    "                    n_channels=cfg.encoder.n_channels,\n",
    "                    filter_channels=cfg.encoder.filter_channels,\n",
    "                    n_heads=cfg.encoder.n_heads,\n",
    "                    n_layers=cfg.encoder.n_layers,\n",
    "                    kernel_size=cfg.encoder.kernel_size,\n",
    "                    p_dropout=cfg.encoder.p_dropout,\n",
    "                    window_size=cfg.encoder.window_size)\n",
    "unit_encoder_dict = torch.load(unit_encoder_path, map_location=lambda storage, loc: storage)\n",
    "unit_encoder.load_state_dict(unit_encoder_dict['model'])\n",
    "_ = unit_encoder.cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization parameters for mel spectrogram\n",
    "decoder_dict = torch.load(cfg.decoder.checkpoint, map_location=lambda loc, storage: loc)\n",
    "\n",
    "mel_max = decoder_dict['mel_max']\n",
    "mel_min = decoder_dict['mel_min']\n",
    "\n",
    "global_phonemizer = phonemizer.backend.EspeakBackend(language='ro',\n",
    "                                                    preserve_punctuation=True,\n",
    "                                                    with_stress=True,\n",
    "                                                    language_switch=\"remove-flags\",\n",
    "                                                    words_mismatch='ignore',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The text gradient scale is responsible for pronunciation and audio quality. \n",
    "# The default value is 1, and increasing the value improves pronunciation accuracy but may reduce speaker similarity. \n",
    "# We recommend starting with 0 and gradually increasing it if the pronunciation is not satisfactory.\n",
    "text_gradient_scale = 1.0\n",
    "\n",
    "# The speaker gradient scale is responsible for speaker similarity. \n",
    "# Increasing the value enhances speaker similarity but may slightly degrade pronunciation and audio quality. \n",
    "# For unique voices, we recommend using a larger value for the speaker gradient scale.\n",
    "spk_gradient_scale = 1.0\n",
    "\n",
    "# We have confirmed that our duration predictor is not accurately following the duration of the reference audio as expected.\n",
    "# As a result, while the reference audio's tone and speaking style are well adapted, there are differences in speech rate. \n",
    "# To address this issue, we use the \"length_scale\" argument as in Grad-TTS to mitigate the discrepancy.\n",
    "# If the value of \"length_scale\" is greater than 1, the speech rate will be slower. \n",
    "# Conversely, if the value is less than 1, the speech rate will be faster.\n",
    "length_scale = 1.0\n",
    "\n",
    "# The number of diffusion steps during sampling refers to the number of iterations performed to improve audio quality.\n",
    "# Generally, larger values lead to better audio quality but slower sampling speeds. \n",
    "# Conversely, smaller values allow for faster sampling but may result in lower audio quality.\n",
    "diffusion_steps = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune(cond_x, y, y_mask, y_lengths, y_max_length, attn, spk_emb, segment_size, n_feats, decoder):\n",
    "    if y_max_length < segment_size:\n",
    "        pad_size = segment_size - y_max_length\n",
    "        y = torch.cat([y, torch.zeros_like(y)[:, :, :pad_size]], dim=-1)\n",
    "        y_mask = torch.cat([y_mask, torch.zeros_like(y_mask)[:, :, :pad_size]], dim=-1)\n",
    "\n",
    "    max_offset = (y_lengths - segment_size).clamp(0)\n",
    "    offset_ranges = list(zip([0] * max_offset.shape[0], max_offset.cpu().numpy()))\n",
    "    out_offset = torch.LongTensor([\n",
    "        torch.tensor(random.choice(range(start, end)) if end > start else 0)\n",
    "        for start, end in offset_ranges\n",
    "    ]).to(y_lengths)\n",
    "\n",
    "    attn_cut = torch.zeros(attn.shape[0], attn.shape[1], segment_size, dtype=attn.dtype, device=attn.device)\n",
    "    y_cut = torch.zeros(y.shape[0], n_feats, segment_size, dtype=y.dtype, device=y.device)\n",
    "    y_cut_lengths = []\n",
    "    for i, (y_, out_offset_) in enumerate(zip(y, out_offset)):\n",
    "        y_cut_length = segment_size + (y_lengths[i] - segment_size).clamp(None, 0)\n",
    "        y_cut_lengths.append(y_cut_length)\n",
    "        cut_lower, cut_upper = out_offset_, out_offset_ + y_cut_length\n",
    "        y_cut[i, :, :y_cut_length] = y_[:, cut_lower:cut_upper]\n",
    "        attn_cut[i, :, :y_cut_length] = attn[i, :, cut_lower:cut_upper]\n",
    "    y_cut_lengths = torch.LongTensor(y_cut_lengths)\n",
    "    y_cut_mask = sequence_mask(y_cut_lengths).unsqueeze(1).to(y_mask)\n",
    "\n",
    "    if y_cut_mask.shape[-1] < segment_size:\n",
    "        y_cut_mask = torch.nn.functional.pad(y_cut_mask, (0, segment_size - y_cut_mask.shape[-1]))\n",
    "\n",
    "    attn = attn_cut\n",
    "    y = y_cut\n",
    "    y_mask = y_cut_mask\n",
    "\n",
    "    # Align encoded text with mel-spectrogram and get cond_y segment\n",
    "    cond_y = torch.matmul(attn.squeeze(1).transpose(1, 2).contiguous(), cond_x.transpose(1, 2).contiguous())\n",
    "    cond_y = cond_y.transpose(1, 2).contiguous()\n",
    "    cond_y = cond_y * y_mask\n",
    "\n",
    "    # Compute loss of score-based decoder\n",
    "    diff_loss, xt = decoder.compute_loss(y, y_mask, cond_y, spk_emb=spk_emb)\n",
    "\n",
    "    return diff_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune loop for each speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for outer_idx, outer_row in reference_speech_samples.iterrows():\n",
    "    # =====================================================================================\n",
    "    path = outer_row[\"path\"]\n",
    "    transcript = outer_row[\"transcript\"]\n",
    "    speaker_id = outer_row[\"speaker_id\"]\n",
    "    print(f\"ID: {speaker_id}\")\n",
    "    # =====================================================================================\n",
    "    # FINETUNE\n",
    "    # Diffusion-based acoutstic model to be finetuned to the current speaker\n",
    "    unitspeech = UnitSpeech(n_feats=cfg.data.n_feats,\n",
    "                            dim=cfg.decoder.dim,\n",
    "                            dim_mults=cfg.decoder.dim_mults,\n",
    "                            beta_min=cfg.decoder.beta_min,\n",
    "                            beta_max=cfg.decoder.beta_max,\n",
    "                            pe_scale=cfg.decoder.pe_scale,\n",
    "                            spk_emb_dim=cfg.decoder.spk_emb_dim)\n",
    "    decoder_dict = torch.load(cfg.decoder.checkpoint, map_location=lambda loc, storage: loc)\n",
    "    unitspeech.load_state_dict(decoder_dict['model'])\n",
    "    _ = unitspeech.cuda().train()\n",
    "\n",
    "    optimizer = torch.optim.Adam(params=unitspeech.parameters(), lr=learning_rate)\n",
    "    if fp16_run:\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    # DATA PROCESSING\n",
    "    wav, sr = librosa.load(path)\n",
    "    wav = torch.FloatTensor(wav).unsqueeze(0)\n",
    "    mel = mel_spectrogram(\n",
    "        wav,\n",
    "        hps_finetune.data.n_fft,\n",
    "        hps_finetune.data.n_feats,\n",
    "        hps_finetune.data.sampling_rate,\n",
    "        hps_finetune.data.hop_length,\n",
    "        hps_finetune.data.win_length,\n",
    "        hps_finetune.data.mel_fmin,\n",
    "        hps_finetune.data.mel_fmax,\n",
    "        center=False,\n",
    "    )\n",
    "    mel_max = decoder_dict['mel_max']\n",
    "    mel_min = decoder_dict['mel_min']\n",
    "    mel = (mel - mel_min) / (mel_max - mel_min) * 2 - 1 \n",
    "    mel = mel.cuda()\n",
    "    # Speaker embedder expects 16KHz audio samples\n",
    "    resample_fn = torchaudio.transforms.Resample(sr, cfg.spkr_embedder.sr).cuda()\n",
    "    wav = resample_fn(wav.cuda())\n",
    "    spk_emb = spk_embedder(wav)\n",
    "    # User speaker embeddings with norm = 1\n",
    "    spk_emb = spk_emb / spk_emb.norm()\n",
    "    # Extract the units and unit durations to be used for fine-tuning.\n",
    "    encoded = unit_extractor(wav.to(\"cuda\")) # => units with f_unit freq: 16Khz\n",
    "    # Upsample unit and durations from f_unit to f_mel\n",
    "    unit, duration = process_unit(encoded, cfg.spkr_embedder.sr, cfg.data.hop_length)\n",
    "    # Reshape the input to match the dimensions and convert it to a PyTorch tensor.\n",
    "    unit = unit.unsqueeze(0).cuda()\n",
    "    duration = duration.unsqueeze(0).cuda()\n",
    "    mel = mel.cuda()\n",
    "    unit_lengths = torch.LongTensor([unit.shape[-1]]).cuda()\n",
    "    mel_lengths = torch.LongTensor([mel.shape[-1]]).cuda()\n",
    "    spk_emb = spk_emb.cuda().unsqueeze(1)\n",
    "    # Prepare unit encoder output for finetuning\n",
    "    with torch.no_grad():\n",
    "        cond_x, x, x_mask = unit_encoder(unit, unit_lengths)\n",
    "    mel_max_length = mel.shape[-1]\n",
    "    mel_mask = sequence_mask(mel_lengths, mel_max_length).unsqueeze(1).to(x_mask)\n",
    "    attn_mask = x_mask.unsqueeze(-1) * mel_mask.unsqueeze(2)\n",
    "    attn = generate_path(duration, attn_mask.squeeze(1))\n",
    "\n",
    "    # Finetune the decoder\n",
    "    for _ in tqdm(range(2_000)):\n",
    "        cond_x = cond_x.detach()\n",
    "        mel = mel.detach()\n",
    "        mel_mask = mel_mask.detach()\n",
    "        mel_lengths = mel_lengths.detach()\n",
    "        spk_emb = spk_emb.detach()\n",
    "        attn = attn.detach()\n",
    "\n",
    "        unitspeech.zero_grad()\n",
    "        \n",
    "        with torch.cuda.amp.autocast(enabled=fp16_run):\n",
    "            diff_loss = fine_tune(cond_x, mel, mel_mask, mel_lengths, mel_max_length, attn, spk_emb, segment_size, hps_finetune.data.n_feats, unitspeech)\n",
    "\n",
    "        loss = sum([diff_loss])\n",
    "\n",
    "        if fp16_run:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            _ = torch.nn.utils.clip_grad_norm_(unitspeech.parameters(), max_norm=1)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            _ = torch.nn.utils.clip_grad_norm_(unitspeech.parameters(), max_norm=1)\n",
    "            optimizer.step()\n",
    "    # =====================================================================================\n",
    "    # NOW USE THE FINETUNED DECODER FOR INFERENCE OVER THE EVALUATION SAMPLES\n",
    "    eval_samples_crnt_speaker = eval_speech_samples[eval_speech_samples[\"speaker_id\"] == speaker_id]\n",
    "    print(eval_samples_crnt_speaker[\"speaker_id\"].value_counts())\n",
    "\n",
    "    # Load the normalization parameters for mel-spectrogram normalization.\n",
    "    mel_max = decoder_dict['mel_max'].cuda()\n",
    "    mel_min = decoder_dict['mel_min'].cuda()\n",
    "    for inner_idx, inner_row in eval_samples_crnt_speaker.iterrows():\n",
    "        inner_path = inner_row[\"path\"]\n",
    "        # Get the sample name from inner_path\n",
    "        crnt_sample = inner_path.split(\"/\")[-1].split(\".\")[0] # no file extension\n",
    "        \n",
    "        inner_transcript = inner_row[\"transcript\"]\n",
    "        inner_speaker_id = inner_row[\"speaker_id\"]\n",
    "        assert inner_speaker_id == speaker_id, \"Running inference on a different speaker ID than current finetuned model.\"\n",
    "        print(f\"Inference on speaker ID {inner_speaker_id}\")\n",
    "        print(f\"\\tPath: {inner_path}\")\n",
    "        print(f\"\\tTranscript: {inner_transcript}\")\n",
    "\n",
    "\n",
    "        phoneme = phonemize(inner_transcript, global_phonemizer)\n",
    "        print(f\"Running inference on: {phoneme}\")\n",
    "        phoneme = cleaned_text_to_sequence(phoneme)\n",
    "        phoneme = intersperse(phoneme, len(symbols))  # add a blank token, whose id number is len(symbols)\n",
    "        phoneme = torch.LongTensor(phoneme).cuda().unsqueeze(0)\n",
    "        phoneme_lengths = torch.LongTensor([phoneme.shape[-1]]).cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_enc, y_dec, _attn = unitspeech.execute_text_to_speech(\n",
    "                phoneme=phoneme,\n",
    "                phoneme_lengths=phoneme_lengths,\n",
    "                spk_emb=spk_emb,\n",
    "                text_encoder=text_encoder,\n",
    "                duration_predictor=duration_predictor,\n",
    "                num_downsamplings_in_unet=num_downsamplings_in_unet,\n",
    "                diffusion_steps=diffusion_steps,\n",
    "                length_scale=length_scale,\n",
    "                text_gradient_scale=text_gradient_scale,\n",
    "                spk_gradient_scale=spk_gradient_scale,\n",
    "            )\n",
    "            mel_generated = ((y_dec + 1) / 2 * (mel_max - mel_min) + mel_min)\n",
    "            synthesized_audio = vocoder.forward(mel_generated).cpu().squeeze().clamp(-1, 1).numpy()\n",
    "\n",
    "            base_path  = \"/workspace/local/evaluation/outputs/with-finetune_AWGN\"\n",
    "            write(f\"{base_path}/{crnt_sample}.wav\", cfg.data.sampling_rate, synthesized_audio)\n",
    "        if inner_idx >= 5:\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unitspeech_3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
